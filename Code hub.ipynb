{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6270f25c",
   "metadata": {},
   "source": [
    "# **LLM Project to Build and Fine Tune a Large Language Model**\n",
    "\n",
    "In today's data-driven world, the ability to process and generate natural language text at scale has become a transformative force across industries. Large Language Models (LLMs) represent a cutting-edge advancement in natural language processing, enabling businesses to extract valuable insights, automate tasks, and enhance user experiences. By harnessing the power of LLMs, organizations can improve customer service, automate content creation, and gain a competitive edge in the digital landscape.\n",
    "\n",
    "This project builds the foundation for Large Language Models by diving deep into the details of their inner workings. Moreover, It shows how to optimize their use through prompt engineering and fine-tuning techniques such as LoRA. \n",
    "\n",
    "Prompt engineering techniques involve crafting specific instructions or queries given to the language model to influence its output will be introduced to guide LLMs in generating desired responses through zero-shot, one-shot, and few-shot inferences.\n",
    "\n",
    "Fine-tuning entails training a pre-trained language model on a specific task or dataset to adapt it for a particular application. It explores full fine-tuning and Parameter Efficient Fine Tuning (PEFT), a technique that optimizes the fine-tuning process by focusing on a subset of the model's parameters, making it more resource-efficient.\n",
    "\n",
    "The project also involves the application of Retrieval Augmented Generation (RAG) using OpenAI's GPT-3.5 Turbo, resulting in the development of a chatbot for online shopping for knowledge grounding. Knowledge grounding with Retrieval Augmented Generation (RAG) is implemented to mitigate hallucinations and provide trustworthy and reliable responses. This is achieved by incorporating information from external sources to validate and support the generated text.\n",
    "\n",
    "For example, in the context of an e-commerce chatbot using RAG, knowledge grounding ensures that product information, availability, and prices are sourced from a trusted database or e-commerce platform. This prevents the chatbot from generating inaccurate or fictional details and instead provides responses based on real-world data.\n",
    "\n",
    "\n",
    "![image](https://images.pexels.com/photos/18069697/pexels-photo-18069697/free-photo-of-an-artist-s-illustration-of-artificial-intelligence-ai-this-illustration-depicts-language-models-which-generate-text-it-was-created-by-wes-cockx-as-part-of-the-visualising-ai-project-l.png?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8406e",
   "metadata": {},
   "source": [
    "## **Learning Outcomes**\n",
    "\n",
    "* Understand Large Language Models (LLMs) and how they work.\n",
    "* Gain practical experience in implementing generative AI projects.\n",
    "* Understand fundamental NLP concepts like RNNs, Transformers, and Attention Mechanism.\n",
    "* Explore tokenization, embeddings, and internal workings of Transformers.\n",
    "* Generate text and summarize dialogues using LLMs.\n",
    "* Learn optimization techniques like Prompt Engineering, Fine-Tuning, and PEFT.\n",
    "* Apply Prompt Engineering techniques for better responses.\n",
    "* Fine-tune LLMs for improved performance on tasks.\n",
    "* Evaluate model performance using the ROUGE metric.\n",
    "* Understand RLHF for improved model output.\n",
    "* Implement Retrieval Augmented Generation (RAG) for knowledge grounding.\n",
    "* Build a chatbot application for online shopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b9628-f0e8-4243-a55e-0365f4dcdb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python \n",
    "!pip install --upgrade pip\n",
    "!pip install transformers\n",
    "!pip install datasets --quiet\n",
    "!pip install torchdata\n",
    "!pip install torch\n",
    "!pip install streamlit\n",
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install unstructured\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "!pip install evaluate==0.4.0\n",
    "!pip install rouge_score==0.1.2\n",
    "!pip install loralib==0.1.1\n",
    "!pip install peft==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c1620-b5bd-43d9-988a-b4775c66ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4466e3-0b30-4e1b-a79a-9f6170370afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM, \n",
    "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a4dce-09af-47d9-9ef3-af01da5437b4",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d0e5b-7fe2-4fcc-a5e6-de43233a04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36582ee-16e1-45e1-b95b-a129748f4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea16bf-6bfc-41be-8192-fb1612c77e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f5711-467a-495c-a9c8-1036f6295060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddaf74-90df-4136-9f09-0e6cae82adfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fa07e-0ae8-4e26-bbb9-7a0d4f9e2954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5896d3b-4727-4b71-bf30-43357ae9a030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684164a-89b5-4f73-8f72-5d2d860ce831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd91e2-a964-4b54-804a-a4a7c0a0f18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09cffe-d011-49c6-b738-1da4fa386590",
   "metadata": {},
   "source": [
    "## **Top-K Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d297e00-67cf-40d7-8342-661202a5e70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=35,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a411c9e-66f8-4456-b274-1ee771988ebe",
   "metadata": {},
   "source": [
    "## **Top-p (nucleus) sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82c10b-2abe-4321-a785-632a4079dae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=35,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5fc26-1a29-4128-9aeb-4dbae20cb4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5498a9-8fa9-4eef-987f-e58b824f0879",
   "metadata": {},
   "source": [
    "# **Dialogue Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3270f6-d29b-49f9-a7e5-72519a7faa43",
   "metadata": {},
   "source": [
    "In this use case, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e802765-6ded-4432-b620-11f9baad4649",
   "metadata": {},
   "source": [
    "Let's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10.000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b5056-8e47-40cb-ad13-b96b9783c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecc2b7-676c-412f-aeb5-912767751f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafeed4a-0ad3-4180-be88-6c6b97085973",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411d4a8-0bc1-42d9-80c2-8e9b1b4d9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_line = \"-\".join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i+1)\n",
    "    print(dash_line)\n",
    "    print('Input Dialogue: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('Baseline Human Summary: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ea871-a66d-4ed2-b1e7-ed703930dfd6",
   "metadata": {},
   "source": [
    "## **FLAN-T5 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23f87-1cf0-4e65-bfc4-3606c32fbac6",
   "metadata": {},
   "source": [
    "<img src=\"images/flan2_architecture.jpg\" width=\"1000\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51b3ea-8b5f-4749-acb4-b36083bebe57",
   "metadata": {},
   "source": [
    "#### Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95e03b-8148-418c-9259-114693afd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9248fbf-1a34-4f29-90d0-070b1b126f2d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35639b0a-188c-4333-917f-2843259a7cea",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24014c68-c37c-4acb-81f3-b08ffb476ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it, Tom?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ca1ec-8652-4eeb-9fc6-3edc65dd806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print(f'ENCODED SENTENCE:\\n {sentence_encoded[\"input_ids\"][0]}')\n",
    "print(f'DECODED SENTENCE: {sentence_decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69b96d-4fb1-4ebd-9834-e0385dca2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dialogues(example_indices, dataset, prompt = \"%s\"):\n",
    "    for i, index in enumerate(example_indices):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "    \n",
    "        input = prompt % (dialogue)\n",
    "        \n",
    "        inputs = tokenizer(input, return_tensors='pt')\n",
    "        pred = model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0]\n",
    "        output = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "    \n",
    "        print(dash_line)\n",
    "        print(f'Example {i+1}')\n",
    "        print(dash_line)\n",
    "        print(f'Input Prompt: \\n {dialogue}')\n",
    "        print(dash_line)\n",
    "        print(f'Baseline Human Summary: \\n {summary}')\n",
    "        print(dash_line)\n",
    "        print(f'Model Generation: \\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b893f-c33f-4084-a61a-eafd80bec0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dialogues(example_indices, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b2c41-41c5-4d5c-b1db-11eb3eefb906",
   "metadata": {},
   "source": [
    "### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0849bb4-948f-48dd-af41-0d0e52252715",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'Summarize the following conversation. \\n%s\\nSummary:'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179486b-f899-4fb7-b04b-c514c5d1e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3417f3-9ca3-425a-820d-ae1bf425a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'Dialogue: \\n%s\\n\\nWhat Happened?'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845923b-e26d-4c1e-9d69-a7a9e4e70450",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e38693-945c-4000-bd83-5912db40047a",
   "metadata": {},
   "source": [
    "# One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f375e-557d-4a14-aefa-ad97f264f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"Dialogue:\\n{dialogue}\\n\\nWhat was going on?\\n{summary}\\n\\n\\n\"\"\"\n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f'Dialogue:\\n{dialogue}\\n\\nWhat was going on?'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70dcf8-1a76-4dbb-ae4f-d94212f73660",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff3608-270f-4415-ab80-84118ff1845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - One Shot:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e758b8a-5e70-4562-987c-6fba74c087c2",
   "metadata": {},
   "source": [
    "# Few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790c048-358f-4e04-9306-57d24d759c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22b0ef-d18a-4915-8b51-1f909bdb6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - Few Shot:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6aed0-085f-4d58-a8e9-1d1b00ae9b91",
   "metadata": {},
   "source": [
    "# Model Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abde7b9-a0f0-4f74-a0aa-eaf4994d428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75512899-12e6-43fd-98f4-9c7d09847510",
   "metadata": {},
   "source": [
    "## Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa593aee-5a96-46de-8160-bb16abe96401",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_dataset_name = \"knkarthick/dialogsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a51cc8-d8d3-48b5-a9d0-1da6ad3ae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(hugging_face_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc43cb5-7ae6-4921-936a-f6962052d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade33c4-b302-4c31-9ff6-c86bfa0efac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        result = f\"trainable model parameters: {trainable_model_params}\\n\"\n",
    "        result += f\"all model parameters: {all_model_params}\\n\"\n",
    "        result += f\"Percentage of model params: {(trainable_model_params/all_model_params)*100}\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1c6a0-f76b-462d-860d-bda4b93a0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad2c79-e6ce-49ad-856a-588f61635429",
   "metadata": {},
   "source": [
    "## Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fefc6a-53a9-4500-b598-a9ca00a3c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Model Generation - Zero Shot: \\n{output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22088e95-6104-486c-be81-81194e1fded1",
   "metadata": {},
   "source": [
    "## Perform Full Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd6c4-4932-49b4-b4a1-32a10ce8b533",
   "metadata": {},
   "source": [
    "### Preprocess the Dialog-Summary dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8012d-3343-4df0-9063-1aebcdd12bc4",
   "metadata": {},
   "source": [
    "Convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with 'Summarize the following conversation' and the start of the summary with 'Summary as follows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd762da7-873a-452a-ae18-de765b32a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007400f-2603-4ed8-bf20-8ec54199576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test\n",
    "# The tokenize_function code is handling all data accross all splits in batches\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb5aa-bf87-45b0-b90d-a01fcf744d7c",
   "metadata": {},
   "source": [
    "To save some time, we will subsample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36a547-96bf-4bfc-8f4b-bbeed5096e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad14994-44d2-4be0-bbd1-4930cf8b0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e16f70-8638-4e8b-971c-16a09ad9dd13",
   "metadata": {},
   "source": [
    "### Fine-Tune the model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297af38-81f9-4a3e-8394-32e2f536b1af",
   "metadata": {},
   "source": [
    "Now utilize the built-in Hugging Face Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e9712-d0a5-43a4-b041-0284f44f057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f8bcb-eea6-48ce-90bc-490ea7175f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc796ce0-a496-428c-85eb-f3b7f18a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('full/').to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c322fc2-062c-443d-a8b1-eddd3ca7741a",
   "metadata": {},
   "source": [
    "## Evaluate the Model Qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51766726-3189-4b08-82cc-ab9ccab2ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model Generation - Fine Tune: \\n{instruct_text_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6bb6-0889-4fe2-9771-a5fc50560215",
   "metadata": {},
   "source": [
    "## Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96e9ae-014c-4e89-b80e-1c97aa78ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a8655-c9cd-4c12-ae09-81ad9532bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_text_output)\n",
    "\n",
    "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b77dd-2618-4d82-bc00-f6c5fda06ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55838ad-c8bc-4e05-a94a-ddff480da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d075dc6-4c89-4ea4-b8bb-0b81e5fe9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"Original Model: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model: \\n{instruct_model_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13167f8b-86c3-4c2f-b7b6-b615b78178c7",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine Tunning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8166e-6506-4ad8-a1b0-b1256965a65e",
   "metadata": {},
   "source": [
    "## Setup the PEFT/LoRA model for Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747ce62-a989-4221-8533-db2d8af27f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba24b4e-5bba-465e-90c0-054bea5a24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q','v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1122785-3957-41a9-b1d9-844d7d7da279",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35033e6-9d34-4874-8f30-da488b5001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./peft-dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da1282-c082-488b-a831-cdffc948e1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c74cb-571b-46be-affc-13c4f995309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    \"peft/\"\n",
    ").to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e485a-a14b-4150-af1d-681509b8a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model Generation - Zero Shot: \\n{peft_text_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaeac7f-aa42-4e44-afd8-83ae481c8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'peft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8def102-fa83-4131-868d-6c8313adc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"Original Model: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model: \\n{instruct_model_results}\")\n",
    "print(f\"Peft Model: \\n{peft_model_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fac159-3d3f-43d8-8bb5-947ff2bcffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run Fashion app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b809f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
